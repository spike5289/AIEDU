{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spike5289/AIEDU/blob/main/notebooks/fine_tune_with_hugging_face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PisUoMdHCDNq"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Fine-tune MedGemma with Hugging Face\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fmedgemma%2Fmain%2Fnotebooks%2Ffine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\">\n",
        "      <img alt=\"HuggingFace logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on HuggingFace\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This notebook demonstrates fine-tuning MedGemma on an image and text dataset for a vision task using Hugging Face libraries.\n",
        "\n",
        "In this guide, you will use Hugging Face's [Transformer Reinforcement Learning (`TRL`)](https://github.com/huggingface/trl) library to train the model with Supervised Fine-Tuning (SFT), utilizing [Quantized Low-Rank Adaptation (QLoRA)](https://arxiv.org/abs/2305.14314) to reduce computational costs while maintaining high performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcamD7TMKjbt"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with sufficient resources to fine-tune the MedGemma model. **Note:** This guide requires a GPU that supports bfloat16 data type and has at least 40 GB of memory.\n",
        "\n",
        "You can run this notebook in Google Colab using an A100 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **â–¾ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **A100 GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2vUYdJKlCZ"
      },
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-4b-it) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syRJ6pl4KpEL"
      },
      "source": [
        "### Configure your HF token\n",
        "\n",
        "Generate a Hugging Face `write` access token by going to [settings](https://huggingface.co/settings/tokens). **Note:** Make sure that the token has write access to push the fine-tuned model to Hugging Face Hub.\n",
        "\n",
        "If you are using Google Colab, add your access token to the Colab Secrets manager to securely store it. If not, proceed to run the cell below to authenticate with Hugging Face.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the ðŸ”‘ Secrets tab in the left panel. <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "2. Create a new secret with the name `HF_TOKEN`.\n",
        "3. Copy/paste your token key into the Value input box of `HF_TOKEN`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w8NaHvwRqL3-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "from huggingface_hub import get_token\n",
        "if get_token() is None:\n",
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFU3B09TKuQf"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgv3DCPIA5p-",
        "outputId": "50c3d610-b7bd-4bc7-9ac1-abd29c830a7c"
      },
      "outputs": [],
      "source": [
        "! uv pip install --upgrade huggingface-hub datasets transformers peft trl evaluate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0dlyBQdpAAD"
      },
      "source": [
        "For this classification task, create a multiple-choice question prompt and preprocess the data into a multimodal conversational format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry64yT0juQBD"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "TISSUE_CLASSES = [\n",
        "    \"A: adipose\",\n",
        "    \"B: background\",\n",
        "    \"C: debris\",\n",
        "    \"D: lymphocytes\",\n",
        "    \"E: mucus\",\n",
        "    \"F: smooth muscle\",\n",
        "    \"G: normal colon mucosa\",\n",
        "    \"H: cancer-associated stroma\",\n",
        "    \"I: colorectal adenocarcinoma epithelium\"\n",
        "]\n",
        "\n",
        "options = \"\\n\".join(TISSUE_CLASSES)\n",
        "PROMPT = f\"What is the most likely tissue type shown in the histopathology image?\\n{options}\"\n",
        "\n",
        "\n",
        "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": PROMPT,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": TISSUE_CLASSES[example[\"label\"]],\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBf5lbmkJ59x"
      },
      "source": [
        "Apply the processing function on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFiPNp30_X6z",
        "outputId": "55ce3330-8f66-4328-ec91-3f33d45e7d80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'image': <PIL.Image.Image image mode=RGB size=224x224>,\n",
              " 'label': 6,\n",
              " 'messages': [{'content': [{'text': None, 'type': 'image'},\n",
              "    {'text': 'What is the most likely tissue type shown in the histopathology image?\\nA: adipose\\nB: background\\nC: debris\\nD: lymphocytes\\nE: mucus\\nF: smooth muscle\\nG: normal colon mucosa\\nH: cancer-associated stroma\\nI: colorectal adenocarcinoma epithelium',\n",
              "     'type': 'text'}],\n",
              "   'role': 'user'},\n",
              "  {'content': [{'text': 'G: normal colon mucosa', 'type': 'text'}],\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = data.map(format_data)\n",
        "\n",
        "# Display a processed data sample\n",
        "data[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOtCBFfSWyS"
      },
      "source": [
        "## Fine-tune the model with LoRA\n",
        "\n",
        "Traditional fine-tuning of large language models is resource-intensive because it requires adjusting billions of parameters. Parameter-Efficient Fine-Tuning (PEFT) addresses this by training a smaller number of parameters. A common PEFT technique is Low-Rank Adaptation (LoRA), which efficiently adapts large language models by training small, low-rank matrices that are added to the original model instead of updating the full-weight matrices. In QLoRA, the base model is quantized to 4-bit before its weights are frozen, then LoRA adapter layers are attached and trained.\n",
        "\n",
        "This notebook demonstrates supervised fine-tuning MedGemma with QLoRA using the `SFTTrainer` from the Hugging Face `TRL` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXh0RtBk_Xc6"
      },
      "source": [
        "### Load model from Hugging Face Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGkKFxVM5Fb"
      },
      "source": [
        "Initialize the quantization configuration and load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-esHCwnQFye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"google/medgemma-4b-it\"\n",
        "\n",
        "# Check if GPU supports bfloat16\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Use right padding to avoid issues during training\n",
        "processor.tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQmPZGy_7NV"
      },
      "source": [
        "### Set up for fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jlPOkUjhH1P"
      },
      "source": [
        "Create a [`LoraConfig`](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig). It will be provided to the `SFTTrainer`, which supports built-in integration with the Hugging Face `PEFT` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCWNm3wtMBqX"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrDwrz6iD3Yl"
      },
      "source": [
        "Define a custom data collator that processes examples containing text and images and returns batches of data in the expected model input format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRPETOTGV81b"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "\n",
        "def collate_fn(examples: list[dict[str, Any]]):\n",
        "    texts = []\n",
        "    images = []\n",
        "    for example in examples:\n",
        "        images.append([example[\"image\"].convert(\"RGB\")])\n",
        "        texts.append(processor.apply_chat_template(\n",
        "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "        ).strip())\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The labels are the input_ids, with the padding and image tokens masked in\n",
        "    # the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask image tokens\n",
        "    image_token_id = [\n",
        "        processor.tokenizer.convert_tokens_to_ids(\n",
        "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "        )\n",
        "    ]\n",
        "    # Mask tokens that are not used in the loss computation\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    labels[labels == image_token_id] = -100\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsaon7JoBq5H"
      },
      "source": [
        "Configure training parameters in an [`SFTConfig`](https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6W_gQmfRXWx"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "num_train_epochs = 1  # @param {type: \"number\"}\n",
        "learning_rate = 2e-4  # @param {type: \"number\"}\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"medgemma-4b-it-sft-lora-crc100k\",            # Directory and Hub repository id to save the model to\n",
        "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
        "    per_device_train_batch_size=4,                           # Batch size per device during training\n",
        "    per_device_eval_batch_size=4,                            # Batch size per device during evaluation\n",
        "    gradient_accumulation_steps=4,                           # Number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
        "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
        "    logging_steps=50,                                        # Number of steps between logs\n",
        "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
        "    eval_strategy=\"steps\",                                   # Evaluate every `eval_steps`\n",
        "    eval_steps=50,                                           # Number of steps between evaluations\n",
        "    learning_rate=learning_rate,                             # Learning rate based on QLoRA paper\n",
        "    bf16=True,                                               # Use bfloat16 precision\n",
        "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                                       # Warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"linear\",                              # Use linear learning rate scheduler\n",
        "    push_to_hub=True,                                        # Push model to Hub\n",
        "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
        "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
        "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnPGToATAFCN"
      },
      "source": [
        "### Fine-tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1rMdSj1Tj1K"
      },
      "source": [
        "Construct an [`SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer) using the previously defined LoRA configuration, custom data collator, and training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWcynpm0MHDc"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=data[\"train\"],\n",
        "    eval_dataset=data[\"validation\"].shuffle().select(range(200)),  # Use subset of validation set for faster run\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoHlpSKKVbDb"
      },
      "source": [
        "Launch the fine-tuning process.\n",
        "\n",
        "**Note**: This may take around 3 hours to run using the default configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viHUr_jJfHCG",
        "outputId": "a57e59a7-ff90-4eff-c12e-7d88cc051d5b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='562' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [562/562 2:56:02, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.520300</td>\n",
              "      <td>0.032325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.103300</td>\n",
              "      <td>0.026194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.093900</td>\n",
              "      <td>0.021495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.088900</td>\n",
              "      <td>0.022999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.085500</td>\n",
              "      <td>0.021236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.082200</td>\n",
              "      <td>0.020522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.083400</td>\n",
              "      <td>0.020533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.081500</td>\n",
              "      <td>0.019521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.079800</td>\n",
              "      <td>0.019584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.078200</td>\n",
              "      <td>0.019550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.077300</td>\n",
              "      <td>0.019425</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=562, training_loss=0.4797590243434567, metrics={'train_runtime': 10581.2693, 'train_samples_per_second': 0.851, 'train_steps_per_second': 0.053, 'total_flos': 7.820988839186266e+16, 'train_loss': 0.4797590243434567})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBotQy2xAU6-"
      },
      "source": [
        "Save the final model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-WxJHtQZJII"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcRXx3qwAdjd"
      },
      "source": [
        "Free up memory before proceeding to evaluate and test the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El9e2_8xZLLi"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf9QkPFAfegp"
      },
      "source": [
        "## Evaluate the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZCfth8Nn3O"
      },
      "source": [
        "### Prepare test dataset\n",
        "\n",
        "The [CRC-VAL-HE-7K](https://zenodo.org/records/1214456) dataset contains image patches from patients with colorectal adenocarcinoma and does not overlap with NCT-CRC-HE-100K. It can be used as the test dataset to evaluate the fine-tuned MedGemma model.\n",
        "\n",
        "**Note:** The full CRC-VAL-HE-7K dataset contains over 7K samples. By default this guide only uses a subset with 1,000 samples to keep the evaluation example small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eziDK33PdB5f"
      },
      "source": [
        "Download and prepare the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv82XT9fVI6y"
      },
      "outputs": [],
      "source": [
        "! wget -nc -q \"https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\"\n",
        "! unzip -q CRC-VAL-HE-7K.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECfWV0ToVZW2"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def format_test_data(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": PROMPT,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example\n",
        "\n",
        "\n",
        "test_data = load_dataset(\"./CRC-VAL-HE-7K\", split=\"train\")\n",
        "test_data = test_data.shuffle(seed=42).select(range(1000))\n",
        "test_data = test_data.map(format_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1Mfl6wWTHk"
      },
      "source": [
        "### Set up for evaluation\n",
        "\n",
        "Load the accuracy and F1 score metrics to evaluate the model's performance on the classfication task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdpSoOtLWkBX"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "# Ground-truth labels\n",
        "REFERENCES = test_data[\"label\"]\n",
        "\n",
        "\n",
        "def compute_metrics(predictions: list[int]) -> dict[str, float]:\n",
        "    metrics = {}\n",
        "    metrics.update(accuracy_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=REFERENCES,\n",
        "    ))\n",
        "    metrics.update(f1_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=REFERENCES,\n",
        "        average=\"weighted\",\n",
        "    ))\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdl6yTAzczn7"
      },
      "source": [
        "Define a postprocessing function to convert responses to integer class labels before computing metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5MEos4Bcy9L"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "# Rename the class names to the tissue classes, `X: tissue type`\n",
        "test_data = test_data.cast_column(\n",
        "    \"label\",\n",
        "    ClassLabel(names=TISSUE_CLASSES)\n",
        ")\n",
        "\n",
        "LABEL_FEATURE = test_data.features[\"label\"]\n",
        "# Mapping to alternative label format, `(X) tissue type`\n",
        "ALT_LABELS = dict([\n",
        "    (label, f\"({label.replace(': ', ') ')}\") for label in TISSUE_CLASSES\n",
        "])\n",
        "\n",
        "\n",
        "def postprocess(prediction: list[dict[str, str]], do_full_match: bool=False) -> int:\n",
        "    response_text = prediction[0][\"generated_text\"]\n",
        "    if do_full_match:\n",
        "        return LABEL_FEATURE.str2int(response_text)\n",
        "    for label in TISSUE_CLASSES:\n",
        "        # Search for `X: tissue type` or `(X) tissue type` in the response\n",
        "        if label in response_text or ALT_LABELS[label] in response_text:\n",
        "            return LABEL_FEATURE.str2int(label)\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh8t3-oUV9X7"
      },
      "source": [
        "### Compute baseline metrics on the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj-8jpcPcPUm"
      },
      "source": [
        "Load the pretrained model using the `pipeline` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ged1Z8Tjb918"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pt_pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Set `do_sample = False` for deterministic responses\n",
        "pt_pipe.model.generation_config.do_sample = False\n",
        "pt_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96XmD5loi6aX"
      },
      "source": [
        "Run batch inference on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cnCCpuSi6aX"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_pipe(\n",
        "    text=test_data[\"messages\"],\n",
        "    images=test_data[\"image\"],\n",
        "    max_new_tokens=40,\n",
        "    batch_size=64,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "pt_predictions = [postprocess(out) for out in pt_outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5slBRejjIOe"
      },
      "source": [
        "Compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Rhs-0LfMPE",
        "outputId": "e0680f00-8bc6-4ecb-ecc9-c23e66871e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline metrics: {'accuracy': 0.43, 'f1': 0.3520150005688922}\n"
          ]
        }
      ],
      "source": [
        "pt_metrics = compute_metrics(pt_predictions)\n",
        "print(f\"Baseline metrics: {pt_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRuNOOAkWFCD"
      },
      "source": [
        "### Compute metrics on the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwzxpFhsA0Yi"
      },
      "source": [
        "Load the base model with the fine-tuned LoRA adapter using the `pipeline` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsy5waU6jG7W"
      },
      "outputs": [],
      "source": [
        "ft_pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=args.output_dir,\n",
        "    processor=processor,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Set `do_sample = False` for deterministic responses\n",
        "ft_pipe.model.generation_config.do_sample = False\n",
        "ft_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
        "# Use left padding during inference\n",
        "processor.tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjw54wGjnWwT"
      },
      "source": [
        "Run batch inference on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjP2BFDcjJxN"
      },
      "outputs": [],
      "source": [
        "ft_outputs = ft_pipe(\n",
        "    text=test_data[\"messages\"],\n",
        "    images=test_data[\"image\"],\n",
        "    max_new_tokens=20,\n",
        "    batch_size=64,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "ft_predictions = [postprocess(out, do_full_match=True) for out in ft_outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_gxaSxRqWjG"
      },
      "source": [
        "Compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YmLRYMRfpon",
        "outputId": "4fce3a7b-fe06-4af2-d0d5-eb39c2b9df33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned metrics: {'accuracy': 0.945, 'f1': 0.9440669568493043}\n"
          ]
        }
      ],
      "source": [
        "ft_metrics = compute_metrics(ft_predictions)\n",
        "print(f\"Fine-tuned metrics: {ft_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Smu13_YlWG"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medgemma/blob/main/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "fine_tune_with_hugging_face.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
